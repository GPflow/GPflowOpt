{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The structure of GPflowOpt\n",
    "*Joachim van der Herten*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, the structure of the GPflowOpt library is explained, including some small examples. First the `Domain` and `Optimizer` interfaces are shortly illustrated, followed by a description of the `BayesianOptimizer`. At the end, a step-by-step walkthrough of the `BayesianOptimizer` is given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "The underlying design principles of GPflowOpt were chosen to address the following task: optimizing problems of the form\n",
    "$$\\underset{\\boldsymbol{x} \\in \\mathcal{X}}{\\operatorname{argmin}} f(\\boldsymbol{x}).$$ The *objective function* $f: \\mathcal{X} \\rightarrow \\mathbb{R}^p$ maps a candidate optimum to a score (or multiple). Here $\\mathcal{X}$ represents the input domain. This domain encloses all candidate solutions to the optimization problem and can be entirely continuous (i.e., a $d$-dimensional hypercube) but may also consist of discrete and categorical parameters. \n",
    "\n",
    "In GPflowOpt, the `Domain` and `Optimizer` interfaces and corresponding subclasses are used to explicitly represent the optimization problem.\n",
    "\n",
    "### Objective function\n",
    "The objective function itself is provided and must be implemented as any python callable (function or object with implemented call operator), accepting a two dimensional numpy array with shape $(n, d)$ as an input, with $n$ the batch size. It returns a tuple of numpy arrays: the first element of the tuple has shape $(n, p)$ and returns the objective scores for each point to evaluate. The second element is the gradient in every point, shaped either $(n, d)$ if we have a single-objective optimization problem, or $(n, d, p)$ in case of a multi-objective function. If the objective function is passed on to a gradient-free optimization method, the gradient is automatically discarded. GPflowOpt provides decorators which handle batch application of a function along the n points of the input matrix, or dealing with functions which accept each feature as function argument.\n",
    "\n",
    "Here, we define a simple quadratic objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fx(X):\n",
    "    X = np.atleast_2d(X)\n",
    "    # Return objective & gradient\n",
    "    return np.sum(np.square(X), axis=1), 2*X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain\n",
    "Then, we represent $\\mathcal{X}$ by composing parameters. This is how a simple continuous square domain is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table id='domain' width=100%><tr><td>Name</td><td>Type</td><td>Values</td></tr><tr><td>x1</td><td>Continuous</td><td>[-2.  2.]</td></tr><tr><td>x2</td><td>Continuous</td><td>[-1.  2.]</td></tr></table>"
      ],
      "text/plain": [
       "<GPflowOpt.domain.Domain at 0x7fcc640c42b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GPflowOpt.domain import ContinuousParameter\n",
    "domain = ContinuousParameter('x1', -2, 2) + ContinuousParameter('x2', -1, 2)\n",
    "domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize\n",
    "Based on the domain and a valid objective function, we can now easily apply one of the included optimizers to optimize objective functions. GPflowOpt defines an intuitive `Optimizer` interface which can be used to specify the domain, the initial point(s), constraints (to be implemented) etc. Some popular optimization approaches are provided.\n",
    "Here is how our function is optimized using one of the available methods of SciPy's minimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.0\n",
       "     jac: array([ 0.,  0.])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 3\n",
       "     nit: 2\n",
       "    njev: 2\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([[ 0.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GPflowOpt.optim import SciPyOptimizer\n",
    "\n",
    "optimizer = SciPyOptimizer(domain, method='SLSQP')\n",
    "optimizer.set_initial([-1,-1])\n",
    "optimizer.optimize(fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we optimize it Monte-Carlo. We can pass the same function as the gradients are automatically discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: array([[ 0.0330519]])\n",
       " message: 'OK'\n",
       "    nfev: 201\n",
       " success: True\n",
       "       x: array([[-0.04434619, -0.17631027]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GPflowOpt.optim import MCOptimizer\n",
    "optimizer = MCOptimizer(domain, 200)\n",
    "optimizer.optimize(fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In Bayesian Optimization (BO), the typical assumption is that $f$ is expensive to evaluate and no gradients are available. The typical approach is to sequentially select a limited set of decisions $\\boldsymbol{x}_0, \\boldsymbol{x}_1, ... \\boldsymbol{x}_{n-1}$ using a sampling policy. Hence each decision $\\boldsymbol{x}_i \\in \\mathcal{X}$ itself is the result of an optimization problem\n",
    "$$\\boldsymbol{x}_i = \\underset{\\boldsymbol{x}}{\\operatorname{argmax}} \\alpha_i(\\boldsymbol{x})$$\n",
    "\n",
    "Each iteration, a function $\\alpha_i$ which is cheap-to-evaluate acts as a surrogate for the expensive function. It is typically a mapping of the predictive distribution of a (Bayesian) model built on all decisions and their corresponding (noisy) evaluations. The mapping introduces an order in $\\mathcal{X}$ to obtain a certain goal. The typical goal within the context of BO is the search for *optimality* or *feasibility* while keeping the amount of required evaluations ($n$) a small number. As we can have several functions $f$ representing objectives and constraints, BO may invoke several models and mappings $\\alpha$. These mappings are typically referred to as *acquisition functions* (or *infill criteria*). GPflowOpt defines an `Acquisition` interface to implement these mappings and provides implementations of some default choices. In combination with a special `Optimizer` implementation for BO, following steps are required for a typical workflow: \n",
    "\n",
    "1) Define the **problem domain**. Its dimensionality matches the input to the objective and constraint functions. (like normal optimization)\n",
    "\n",
    "2) Specify the **(GP) models** for the constraints and objectives. This involves choice of kernels, priors, fixes, transforms... this step follows the standard way of setting up GPflow models. GPflowOpt does not further wrap models hence it is possible to implement custom models in GPflow and use them directly in GPflowOpt\n",
    "\n",
    "3) Set up the **acquisition function(s)** using the available built-in implementations in GPflowOpt, or design your own by implementing the `Acquisition` interface.\n",
    "\n",
    "4) Set up an **optimizer** for the acquisition function.\n",
    "\n",
    "5) Run the **high-level** `BayesianOptimizer` which implements a typical BO flow. `BayesianOptimizer` in itself is compliant with the `Optimizer` interface. Exceptionally, the `BayesianOptimizer` requires that the objective function returns **no gradients**.\n",
    "\n",
    "Alternatively, advanced users requiring finer control can easily implement their own flow based on the low-level interfaces of GPflowOpt, as the coupling between these objects was intentionally kept loose.\n",
    "\n",
    "As illustration of the described flow, the previous example is optimized using Bayesian optimization instead, with the well-known Expected Improvement acquisition function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: array([ 0.0008643])\n",
      " message: 'OK'\n",
      "    nfev: 15\n",
      " success: True\n",
      "       x: array([[ 0.        ,  0.02939904]])\n"
     ]
    }
   ],
   "source": [
    "from GPflowOpt.bo import BayesianOptimizer\n",
    "from GPflowOpt.design import FactorialDesign\n",
    "from GPflowOpt.acquisition import ExpectedImprovement\n",
    "import GPflow\n",
    "\n",
    "# The Bayesian Optimizer does not expect gradients to be returned\n",
    "def fx(X):\n",
    "    X = np.atleast_2d(X)\n",
    "    # Return objective & gradient\n",
    "    return np.sum(np.square(X), axis=1)[:,None]\n",
    "\n",
    "    \n",
    "X = FactorialDesign(2, domain).generate()\n",
    "Y = fx(X)\n",
    "\n",
    "# initializing a standard BO model, Gaussian Process Regression with\n",
    "# Matern52 ARD Kernel\n",
    "model = GPflow.gpr.GPR(X,Y,GPflow.kernels.Matern52(2, ARD=True))\n",
    "alpha = ExpectedImprovement(model)\n",
    "\n",
    "# Now we must specify an optimization algorithm to optimize the acquisition \n",
    "# function, each iteration. \n",
    "acqopt = SciPyOptimizer(domain)\n",
    "\n",
    "# Now create the Bayesian Optimizer\n",
    "optimizer = BayesianOptimizer(domain, alpha, optimizer=acqopt)\n",
    "with optimizer.silent():\n",
    "    r = optimizer.optimize(fx, n_iter=15)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brief snippet code starts from a 2-level grid (corner points of the domain) and uses a GP model to model the response surface of the objective function. The `BayesianOptimizer` follows the same interface as other optimizers and is initialized with a domain, the acquisition function and an additional optimization method to optimize the acquisition function each iteration. Finally, the optimizer performs 10 iterations to optimize fx.\n",
    "\n",
    "The code to evaluate the acquisition function on the model is written in TensorFlow, allowing gradient-based optimization without additional effort due to the automated differentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step description of the BayesianOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Prior to running `BayesianOptimizer.optimize()`, the acquisition function is initialized with an underlying model. Any data previously included in the model (through the `GPModel.__init__` constructor in GPflow) is used as initial data. When `optimize(function, n_iter)` is called:\n",
    "\n",
    "1) Any data points returned by `get_initial()` are evaluated. Afterwards the evaluated points are added to the models by calling `_update_model_data()`. \n",
    "\n",
    "2) `n_iter` iterations are performed. Each iteration the acquisition function is optimized, and the models are updated by calling `_update_model_data()`\n",
    "\n",
    "The updating of a model through `_update_model_data()` calls `set_data(X, Y)` on the acquisition function. This covers following aspects:\n",
    "\n",
    "* `GPModel.X` and `GPModel.Y` are updated\n",
    "\n",
    "* Each of the contained models are returned to the state when the acquisition function was initialized and optimized. If the `optimize_restarts` parameter of the `Acquisition.__init__()` was set to $n>1$, the state of the model is randomized and optimized $n-1$ times. Finally, the state resulting in the best `log_likelihood()` is the new model state   \n",
    "\n",
    "* Call `Acquisition.setup()` to perform any pre-calculation of quantities independent of candidate points, which can be used in `build_acquisition()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GPflow tree\n",
    "The `Acquisition` interface, mapping the belief of the model(s) to a score indicating areas of optimality/feasibility, is implemented as part of the [GPflow tree structure](https://gpflow.readthedocs.io/en/latest/notebooks/structure.html). More specifically it implements the `Parameterized` interface permitting the use of the useful `AutoFlow` decorator. The `build_acquisition()` method to be implemented by subclasses is a TensorFlow method, allowing automated differentiation of the acquisition function which enables gradient-based optimization thereof (not of the objective!). It may directly access the graph for computing the predictive distribution of a model by calling `build_predict()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}